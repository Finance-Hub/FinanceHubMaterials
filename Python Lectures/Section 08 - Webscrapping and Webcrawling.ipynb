{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 9 - Web Scraping and Web Crawling\n",
    "Data from websites are not always neatly organized. We call this **unstructured data**. Most webpages are rendered in **HTML**, a programming language designed for layout and formatting. But in order to understand how to work with unstructured data from websites we need to know a little bit of HTML. CodeAcademy has a great free [HTML tutorial](https://www.codecademy.com/learn/learn-html), which covers even more than what we need for this section. We are also going to use a little bit of **regular expressions** (or RegEx), which is a special string describing a search pattern. And here is a great [RegEx tutorial](https://regexone.com/).\n",
    "\n",
    "Now that we know the basics of HTML, you should also learn how to **open the source code panel** of your web browser. If you are using Google Chrome, just press F12. If you are using Safari on a Mac, you need to **enable the developer menu**:\n",
    "1. Click on *Safari Menu* > *Preferences* > *Advanced*\n",
    "1. Check the *\"show Develop menu in the menu bar\"*\n",
    "1. Now there a is a drop down menu called *Develop*, with the *view source* option.\n",
    "\n",
    "In this notebook we are going to use some new libraries:\n",
    "* **`requests`**: Allows to grab the source code and other characteristics of a webpage.\n",
    "* **`BeaultifulSoup4`**: Interprets text as sorce code and lets you grab the parts you want.\n",
    "* **`re`**: The RegEx module that is already built in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, our target is going to the **G1 website**.\n",
    "\n",
    "https://g1.globo.com/\n",
    "\n",
    "We can use the requests library to `get` the contents of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('http://g1.globo.com')\n",
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notice that the `get` method returns a specific object of the `requests` library. Let's take a look at the atributes of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a few of these attributes on the next cell. The most important for us is going to be `.text` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use RegEx to search content in this page. For example, let's **grab all the URLs from G1 website**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_html_url = '''href=[\"'](.[^\"']+)[\"']'''  # RegEx for URLs\n",
    "re.findall(regex_html_url, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that you can now write a program to visit all the avaible URLs from G1 home page and turn your webscrapper into a webcrawler.\n",
    "\n",
    "Now, let's say we want to **grab the text from all the headlines**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "response = requests.get('http://g1.globo.com')\n",
    "headlines = bs(response.content, 'html.parser').find_all('a', class_='feed-post-link gui-color-primary gui-color-hover')\n",
    "\n",
    "print(len(headlines))\n",
    "\n",
    "headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To grab the contents inside a HTML tag, use the `.get_text` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(headlines[0]), '\\n')\n",
    "\n",
    "print(headlines[0])\n",
    "\n",
    "headlines[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(headlines)):\n",
    "    print(headlines[i].get_text(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Scraping Emails\n",
    "\n",
    "[Students from the masters of economics in PUC-Rio](http://www.econ.puc-rio.br/pessoas/alunos-mestrado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = requests.get('http://www.econ.puc-rio.br/pessoas/alunos-mestrado')\n",
    "\n",
    "regExpEmail = '[\\w\\.-]+@[\\w\\.-]+' # RegEx for email\n",
    "re.findall(regExpEmail, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you click on the name of a student, you can see that the URL ends with something that looks like an ID number. The link for the first one is \n",
    "\n",
    "http://www.econ.puc-rio.br/pessoas/perfil/1\n",
    "\n",
    "If you keep adding 1 to this ID number, you will go through everybody that is registered in the department of economics from PUC-Rio.\n",
    "\n",
    "Not all the ID numbers are valid. For example, number 17 returns an error\n",
    "\n",
    "http://www.econ.puc-rio.br/pessoas/perfil/17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get('http://www.econ.puc-rio.br/pessoas/perfil/17').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use this in our favor. Whenever the string \"fatal error\" is in the response we are going to skip that ID number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall('Fatal error', requests.get('http://www.econ.puc-rio.br/pessoas/perfil/17').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the page exists, we use `beatifulsoup` to grab the HTML tag of the person's name and a regular expression search to get the email. We are searching for different information using different methods for a reason. The person's name is always under the `<h5>` HTML tag, while several other information are under the same tag `<a>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('http://www.econ.puc-rio.br/pessoas/perfil/18')\n",
    "\n",
    "soup = bs(response.content.decode('utf-8','ignore'), 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_a = soup.find_all('a')\n",
    "for p in all_a:\n",
    "    print(p.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find_all('h5')[0].get_text())\n",
    "\n",
    "print(re.findall(regExpEmail , response.text)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all we have to do is iterate on the ID of the URL and save the name and email, but we have to handle two types of errors:\n",
    "* We skip the iteration if the page is not avilable. To handle this we check if the string \"fatal error\" is on the page.\n",
    "* If there is no email on the page, we have to save a blank. To handle this problem we use the `try-except` clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creates empty lists that are going to store the data\n",
    "Nomes = []\n",
    "Emails = []\n",
    "\n",
    "# loops for IDs from 1 to 2000\n",
    "for i in tqdm(range(1, 3000)):    \n",
    "    \n",
    "    #if i%250 == 0:\n",
    "     #   print('buscando pagina', i)\n",
    "    \n",
    "    response = requests.get('http://www.econ.puc-rio.br/pessoas/perfil/' + str(i))\n",
    "    \n",
    "    if 'Fatal error' in re.findall('Fatal error', response.text):\n",
    "        continue  # skips to the next iteration\n",
    "    else:\n",
    "        nome_i = bs(response.content.decode('utf-8','ignore'), 'html.parser').find_all('h5')[0].text\n",
    "        \n",
    "        try:\n",
    "            email_i = re.findall(regExpEmail, response.text)[0]\n",
    "        except IndexError:\n",
    "            email_i = ''\n",
    "  \n",
    "        Nomes.append(nome_i)\n",
    "        Emails.append(email_i)\n",
    "        \n",
    "EmailsPUC = pd.DataFrame(data={'Nomes':Nomes, 'Emails':Emails}).sort_values('Nomes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EmailsPUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EmailsPUC[EmailsPUC['Nomes'].str.contains('Gustavo')].dropna().sort_values('Nomes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to protect yourself from webscraping is to not write your email **explicitly**. Here is an example from Marcos Lopez de Prado [website](http://www.quantresearch.info/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Query Strings: URLs with Parameters\n",
    "Everything after \"?\" is the query string and it is meant to contain data that does not fit within a URL’s normal hierarchical path structure\n",
    "\n",
    "https://www.google.com.br/search?q=pesquisa+eleitoral+bolsonaro&num=3&as_sitesearch=g1.com.br\n",
    "\n",
    "* query string comes at the end of a URL, starting with a single question mark, “?”.\n",
    "* Parameters are provided as key-value pairs and separated by an ampersand, “&”.\n",
    "* The key and value are separated using an equals sign, “=”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.google.com.br/search?q=pesquisa+eleitoral+bolsonaro&num=3&as_sitesearch=g1.com.br'\n",
    "res = requests.get(url)\n",
    "res.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want an easy way to change the parameters of our search. Luckily, the `requests` library already has a functionality for URL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.google.com.br/search'\n",
    "\n",
    "param_dict = {'q': 'programa+de+governo', \n",
    "              'num': '3', \n",
    "              'as_sitesearch': 'g1.com.br'}\n",
    "\n",
    "response = requests.get(url, params=param_dict)\n",
    "\n",
    "response.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Example - Derivatives Settlement Prices from B3\n",
    "An applied example of this is going to be relevant for is scraping sttlement prices from the B3 derivatives.\n",
    "\n",
    "http://www2.bmf.com.br/pages/portal/bmfbovespa/lumis/lum-sistema-pregao-enUS.asp?Data=10/01/2018&Mercadoria=DOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www2.bmf.com.br/pages/portal/bmfbovespa/lumis/lum-sistema-pregao-enUS.asp'\n",
    "param_dict = {'Data': '10/01/2018', 'Mercadoria': 'DOL'}\n",
    "r = requests.get(url, params=param_dict)\n",
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Webdrivers\n",
    "First we need to install the [**selenium webdriver library**](https://www.seleniumhq.org/projects/webdriver/). This library allows you to command the web browsers. We will use **Google Chrome** as our web browser, so we need the [**Chrome Driver**](https://sites.google.com/a/chromium.org/chromedriver/downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import datetime as dt\n",
    "import time as time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_options = webdriver.ChromeOptions()\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\gamarante\\Desktop\\chromedriver', \n",
    "                           chrome_options=driver_options)\n",
    "\n",
    "browser.get(r'https://www3.bcb.gov.br/expectativas/publico/consulta/serieestatisticas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select the indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = r'//*[@id=\"indicador\"]/option[5]'\n",
    "browser.find_element_by_xpath(xpath).click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the price index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = r'//*[@id=\"grupoIndicePreco:opcoes_5\"]'\n",
    "browser.find_element_by_xpath(xpath).click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = r'//*[@id=\"calculo\"]/option[3]'\n",
    "browser.find_element_by_xpath(xpath).click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the periodicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = r'//*[@id=\"periodicidade\"]/option[3]'\n",
    "browser.find_element_by_xpath(xpath).click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_date = dt.date(2018, 1, 10)\n",
    "xpath = r'//*[@id=\"tfDataInicial1\"]'\n",
    "browser.find_element_by_xpath(xpath).send_keys(initial_date.strftime('%d/%m/%Y'))\n",
    "\n",
    "end_date = dt.date(2018, 11, 16)\n",
    "xpath = r'//*[@id=\"tfDataFinal2\"]'\n",
    "browser.find_element_by_xpath(xpath).send_keys(end_date.strftime('%d/%m/%Y'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill initial year and final year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = r'//*[@id=\"form4\"]/div[2]/table/tbody[3]/tr/td[2]/select/option[2]'\n",
    "browser.find_element_by_xpath(xpath).click()\n",
    "\n",
    "# trick - always grabs the last element\n",
    "xpath = r'//*[@id=\"form4\"]/div[2]/table/tbody[3]/tr/td[4]/select'\n",
    "selection = browser.find_element_by_xpath(xpath)\n",
    "selection.click()\n",
    "options = selection.find_elements_by_tag_name('option')\n",
    "options[len(options) - 1].click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "click the download button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = r'//*[@id=\"btnXLSa\"]'\n",
    "browser.find_element_by_xpath(xpath).click()\n",
    "\n",
    "download_save_time = dt.datetime.now()  # saves the time the file was downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we close the browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the default download directory\n",
    "\n",
    "Probably only works on windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "username = os.getlogin()\n",
    "download_path = r'C:\\Users\\%(user)s\\Downloads' % {'user': username}\n",
    "download_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the walk method\n",
    "dirpath\n",
    "dirname\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list(os.walk(download_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for the files in the download path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (dirpath, dirnames, filenames) in os.walk(download_path):\n",
    "    \n",
    "    for f in filenames:\n",
    "    \n",
    "        if 'Séries de estatísticas' in f:\n",
    "            file_save_time = os.path.getmtime(dirpath + '\\\\' + f)\n",
    "            file_save_time = dt.datetime.fromtimestamp(file_save_time)\n",
    "            \n",
    "            if file_save_time > download_save_time:\n",
    "                file_path = dirpath + '\\\\' + f\n",
    "                \n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read the file and clean the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(file_path, skiprows=1, na_values=[' '])\n",
    "\n",
    "df['Data'] = pd.to_datetime(df['Data'], dayfirst=True)\n",
    "df = df.set_index('Data')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df.plot(figsize=(15, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All together now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_options = webdriver.ChromeOptions()\n",
    "\n",
    "browser = webdriver.Chrome(r'\\\\bw2k1201app\\groupbrw$\\BWGI\\Thiago Barros\\Drivers\\chromedriver', \n",
    "                           chrome_options=driver_options)\n",
    "\n",
    "browser.get(r'https://www3.bcb.gov.br/expectativas/publico/consulta/serieestatisticas')\n",
    "\n",
    "xpath = r'//*[@id=\"indicador\"]/option[5]'\n",
    "browser.find_element_by_xpath(xpath).click()\n",
    "\n",
    "xpath = r'//*[@id=\"grupoIndicePreco:opcoes_5\"]'\n",
    "browser.find_element_by_xpath(xpath).click()\n",
    "\n",
    "xpath = r'//*[@id=\"calculo\"]/option[3]'\n",
    "browser.find_element_by_xpath(xpath).click()\n",
    "\n",
    "xpath = r'//*[@id=\"periodicidade\"]/option[3]'\n",
    "browser.find_element_by_xpath(xpath).click()\n",
    "\n",
    "initial_date = dt.date(2018, 1, 10)\n",
    "xpath = r'//*[@id=\"tfDataInicial1\"]'\n",
    "browser.find_element_by_xpath(xpath).send_keys(initial_date.strftime('%d/%m/%Y'))\n",
    "\n",
    "end_date = dt.date(2018, 11, 16)\n",
    "xpath = r'//*[@id=\"tfDataFinal2\"]'\n",
    "browser.find_element_by_xpath(xpath).send_keys(end_date.strftime('%d/%m/%Y'))\n",
    "\n",
    "xpath = r'//*[@id=\"form4\"]/div[2]/table/tbody[3]/tr/td[2]/select/option[2]'\n",
    "browser.find_element_by_xpath(xpath).click()\n",
    "\n",
    "# trick - always grabs the last element\n",
    "xpath = r'//*[@id=\"form4\"]/div[2]/table/tbody[3]/tr/td[4]/select'\n",
    "selection = browser.find_element_by_xpath(xpath)\n",
    "selection.click()\n",
    "options = selection.find_elements_by_tag_name('option')\n",
    "options[len(options) - 1].click()\n",
    "\n",
    "xpath = r'//*[@id=\"btnXLSa\"]'\n",
    "browser.find_element_by_xpath(xpath).click()\n",
    "\n",
    "download_save_time = dt.datetime.now()  # saves the time the file was downloaded\n",
    "\n",
    "time.sleep(6)  # give some time for the download to finish\n",
    "\n",
    "browser.close()\n",
    "\n",
    "username = os.getlogin()\n",
    "download_path = r'C:\\Users\\%(user)s\\Downloads' % {'user': username}\n",
    "\n",
    "for (dirpath, dirnames, filenames) in os.walk(download_path):\n",
    "    \n",
    "    for f in filenames:\n",
    "    \n",
    "        if 'Séries de estatísticas' in f:\n",
    "            file_save_time = os.path.getmtime(dirpath + '\\\\' + f)\n",
    "            file_save_time = dt.datetime.fromtimestamp(file_save_time)\n",
    "            \n",
    "            if file_save_time > download_save_time:\n",
    "                file_path = dirpath + '\\\\' + f\n",
    "                \n",
    "df = pd.read_excel(file_path, skiprows=1, na_values=[' '])\n",
    "\n",
    "df['Data'] = pd.to_datetime(df['Data'], dayfirst=True)\n",
    "df = df.set_index('Data')\n",
    "\n",
    "df.plot(figsize=(15, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
