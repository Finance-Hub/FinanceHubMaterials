{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensamble methods\n",
    "\n",
    "#### by Gustavo Soares\n",
    "\n",
    "In this notebook you will apply a few things you learned in the [FinanceHub's Python lectures](https://github.com/Finance-Hub/FinanceHubMaterials/tree/master/Python%20Lectures) as well as in the [FinanceHub's Quantitative Finance Lectures](https://github.com/Finance-Hub/FinanceHubMaterials/tree/master/Quantitative%20Finance%20Lectures). In particular, we will make use what we have learned about the [Bootstrap](https://github.com/Finance-Hub/FinanceHubMaterials/blob/master/Quantitative%20Finance%20Lectures/bootstrap_in_finance.ipynb).\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The [Bootstrap](https://github.com/Finance-Hub/FinanceHubMaterials/blob/master/Quantitative%20Finance%20Lectures/bootstrap_in_finance.ipynb) is used in many situations in which it is hard or even impossible to directly compute the standard deviation of a quantity of interest. However, it is an extremely powerful tool for prediction as well.\n",
    "\n",
    "Often time, prediction methods suffer from high variance and low accuracy. This means that if we split the training data into two parts at random, and fit a decision tree to both halves, the results that we get could be quite different. **Bootstrap aggregation**, or **bagging**, is a general-purpose procedure for reducing the bagging variance of a statistical learning method. It is particularly useful and frequently used in the context of [decision trees](https://github.com/Finance-Hub/FinanceHubMaterials/blob/master/Quantitative%20Finance%20Lectures/tree_based_methods_in_fx.ipynb).\n",
    "\n",
    "Let's get started by importing a few things and get right away to an example.\n",
    "\n",
    "### FX trading example\n",
    "\n",
    "Here we will use the example of FX trading to illustrate how to use bagging for predictions. We have discussed previously the three main types of signals in FX trading:\n",
    "[carry](https://github.com/Finance-Hub/FinanceHubMaterials/blob/master/Quantitative%20Finance%20Lectures/carry.ipynb), [momentum](https://github.com/Finance-Hub/FinanceHubMaterials/blob/master/Quantitative%20Finance%20Lectures/time_series_momentum.ipynb) and [value](https://github.com/Finance-Hub/FinanceHubMaterials/blob/master/Quantitative%20Finance%20Lectures/ppp_value_in_fx.ipynb). So, let's start by getting the data on these three types of signals.\n",
    "\n",
    "For each signal $s$ for $s \\in \\{carry,momentum,value\\}$ we have a value $x_{s,i,t}$ containing the signal $s$ for currency $i$ at time $t$ already appropriately lagged. Remember, that we need to make sure $x_{s,i,t}$ is only using information that was available at the time of trading to predict h-period returns from time $t$ to some time in the future $t+h$. So, the value $x_{s,i,t}$ needs to be calculated using information prior to $t$. Here, we lag the information set by one period and calculate $x_{s,i,t}$ only with information contained in $\\mathscr{I}_{t-1}$.\n",
    "\n",
    "We also discussed how to construct [FX trackers](https://github.com/Finance-Hub/FinanceHubMaterials/blob/master/Quantitative%20Finance%20Lectures/creating_fx_time_series_fh.ipynb) for each currency against the USD. Here, we will just upload the data on the FX trackers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carry data has 24 currencies and 4973 dates\n",
      "momentum data has 24 currencies and 4968 dates\n",
      "value data has 24 currencies and 4973 dates\n",
      "trackers data has 24 currencies and 5220 dates\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "carry_df = pd.read_excel(r'fx_trackers_and_signals.xlsx',sheet_name='carry',index_col=0)\n",
    "print('carry data has %s currencies and %s dates' % (carry_df.shape[1],carry_df.shape[0]))\n",
    "mom_df = pd.read_excel(r'fx_trackers_and_signals.xlsx',sheet_name='momentum',index_col=0)\n",
    "print('momentum data has %s currencies and %s dates' % (mom_df.shape[1],mom_df.shape[0]))\n",
    "value_df = pd.read_excel(r'fx_trackers_and_signals.xlsx',sheet_name='value',index_col=0)\n",
    "print('value data has %s currencies and %s dates' % (value_df.shape[1],value_df.shape[0]))\n",
    "trackers_df = pd.read_excel(r'fx_trackers_and_signals.xlsx',sheet_name='trackers',index_col=0)\n",
    "print('trackers data has %s currencies and %s dates' % (trackers_df.shape[1],trackers_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable we want to predict is\n",
    "$$\n",
    "r_{i,t+h} \\equiv \\frac{I_{t+h}}{I_{t}}-1\n",
    "$$\n",
    "\n",
    "which contains the returns of currency $i$ over the period between $t$ and $t+h$ as measured by the percentage change in the the currency tracker level $I_{t}$ over the period. This assumes that we traded at level $I_{t}$ at inception and closed the position $I_{t+h}$. Let's use `Pandas` to create a dataframe containing these returns for $h=21$ business days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "returns data has 24 currencies and 2241 dates\n"
     ]
    }
   ],
   "source": [
    "h = 21\n",
    "# note the use of the .shift(-h) method below to make sure that on the index t we have the returns from t to t+h\n",
    "returns_df = trackers_df.pct_change(h).shift(-h).dropna()\n",
    "print('returns data has %s currencies and %s dates' % (returns_df.shape[1],returns_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling the data\n",
    "\n",
    "Let's start by poolling the data. That is we create a set of variables we want to predict:\n",
    "\n",
    "$$\n",
    "y_{i,t} = r_{i,t+h}\n",
    "$$\n",
    "\n",
    "and a $3 \\times 1$ vector of signals containing the signals for currency $i$ at time $t$, $X_{i,t} = [x_{carry,i,t},x_{momentum,i,t},x_{value,i,t}]'$. This vector $X_{i,t}$ will be used to predict the future returns $y_{i,t} = r_{i,t+h}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_dict = dict(zip([ccy for ccy in returns_df.columns],range(len(returns_df.columns))))\n",
    "groups = pd.DataFrame()\n",
    "pooled_data = pd.DataFrame()\n",
    "for ccy in returns_df.columns:\n",
    "    # get future returns\n",
    "    y = returns_df[ccy].to_frame('returns')\n",
    "    # get the three signals\n",
    "    X_carry = carry_df[ccy].dropna().to_frame('carry')\n",
    "    X_mom = mom_df[ccy].dropna().to_frame('mom')\n",
    "    X_value = value_df[ccy].dropna().to_frame('value')\n",
    "    # make sure the signals are lined up and fill the nan's with the last obs in case there are some\n",
    "    X = pd.concat([X_carry,X_mom,X_value],join='outer',axis=1,sort=True).fillna(method='ffill').dropna()\n",
    "    \n",
    "    Z = (X-X.shift(1).ewm(halflife=63).mean())/X.shift(1).ewm(halflife=63).std()\n",
    "    # make sure the dates of the signals and future returns line up\n",
    "    yZ = pd.concat([y,Z],axis=1,sort=True).dropna()\n",
    "    \n",
    "    pooled_data = pooled_data.append(yZ)\n",
    "    \n",
    "    group = pd.DataFrame(index=yZ.index,columns=['group'],data=group_dict[ccy])\n",
    "    groups = groups.append(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of only using the three signals, $[x_{carry,i,t},x_{momentum,i,t},x_{value,i,t}]'$ for prediction, let's also use their interactions and squares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>carry</th>\n",
       "      <th>mom</th>\n",
       "      <th>value</th>\n",
       "      <th>carry^2</th>\n",
       "      <th>carry mom</th>\n",
       "      <th>carry value</th>\n",
       "      <th>mom^2</th>\n",
       "      <th>mom value</th>\n",
       "      <th>value^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.217493</td>\n",
       "      <td>-0.215423</td>\n",
       "      <td>-0.511210</td>\n",
       "      <td>1.482290</td>\n",
       "      <td>-0.262276</td>\n",
       "      <td>-0.622394</td>\n",
       "      <td>0.046407</td>\n",
       "      <td>0.110126</td>\n",
       "      <td>0.261335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.854565</td>\n",
       "      <td>-0.174336</td>\n",
       "      <td>-0.595210</td>\n",
       "      <td>0.730281</td>\n",
       "      <td>-0.148982</td>\n",
       "      <td>-0.508645</td>\n",
       "      <td>0.030393</td>\n",
       "      <td>0.103767</td>\n",
       "      <td>0.354275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.565515</td>\n",
       "      <td>-0.402217</td>\n",
       "      <td>-0.304515</td>\n",
       "      <td>0.319807</td>\n",
       "      <td>-0.227460</td>\n",
       "      <td>-0.172208</td>\n",
       "      <td>0.161779</td>\n",
       "      <td>0.122481</td>\n",
       "      <td>0.092729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.570330</td>\n",
       "      <td>-0.461571</td>\n",
       "      <td>-0.407267</td>\n",
       "      <td>0.325276</td>\n",
       "      <td>-0.263248</td>\n",
       "      <td>-0.232276</td>\n",
       "      <td>0.213048</td>\n",
       "      <td>0.187983</td>\n",
       "      <td>0.165866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.909194</td>\n",
       "      <td>-0.302033</td>\n",
       "      <td>-0.515772</td>\n",
       "      <td>0.826633</td>\n",
       "      <td>-0.274606</td>\n",
       "      <td>-0.468936</td>\n",
       "      <td>0.091224</td>\n",
       "      <td>0.155780</td>\n",
       "      <td>0.266020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              1     carry       mom     value   carry^2  carry mom  \\\n",
       "2011-01-18  1.0  1.217493 -0.215423 -0.511210  1.482290  -0.262276   \n",
       "2011-01-19  1.0  0.854565 -0.174336 -0.595210  0.730281  -0.148982   \n",
       "2011-01-20  1.0  0.565515 -0.402217 -0.304515  0.319807  -0.227460   \n",
       "2011-01-21  1.0  0.570330 -0.461571 -0.407267  0.325276  -0.263248   \n",
       "2011-01-24  1.0  0.909194 -0.302033 -0.515772  0.826633  -0.274606   \n",
       "\n",
       "            carry value     mom^2  mom value   value^2  \n",
       "2011-01-18    -0.622394  0.046407   0.110126  0.261335  \n",
       "2011-01-19    -0.508645  0.030393   0.103767  0.354275  \n",
       "2011-01-20    -0.172208  0.161779   0.122481  0.092729  \n",
       "2011-01-21    -0.232276  0.213048   0.187983  0.165866  \n",
       "2011-01-24    -0.468936  0.091224   0.155780  0.266020  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = PolynomialFeatures()\n",
    "X.fit_transform(pooled_data.iloc[:,1:])\n",
    "features_names = X.get_feature_names(pooled_data.columns[1:])\n",
    "X = X.fit_transform(pooled_data.iloc[:,1:])\n",
    "X = pd.DataFrame(index=pooled_data.index,data=X,columns=features_names)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we predicting future returns $y_{i,t} = r_{i,t+h}$ with the  $X_{i,t}$ but using a second-order polinomial instad of simply using a simple linear function.\n",
    "\n",
    "### Current state\n",
    "\n",
    "Suppose I observe the following vector $X_{i,T}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.268358  , -0.112417  ,  0.212429  ,  0.07201602,\n",
       "         0.030168  , -0.05700702,  0.01263758, -0.02388063,  0.04512608]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_signal = -0.268358\n",
    "m_signal = -0.112417\n",
    "v_signal = 0.212429\n",
    "X0 = PolynomialFeatures()\n",
    "X0 = X0.fit_transform([[c_signal,m_signal,v_signal]])\n",
    "X0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting with a Support Vector Classifier\n",
    "\n",
    "Support vector classifiers (SVC) is a popular technique for classification. SVCs have been shown to perform well in a variety of settings, and are often considered one of the best “out of the box” classifiers. SVC performs particularly well relative to other methods when we have high dimensional spaces. SVCs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation. A good discusion on SVM can be found in Chapter 9 of [James, Witten, Hastie, and Tibshirani (2013)](http://faculty.marshall.usc.edu/gareth-james/ISL/).  Let's use the [support vector machines package in scikit-learn](https://scikit-learn.org/stable/modules/svm.html#svm-classification) to carry out the SVC predictions as we have discussed in our [previous lecture](https://github.com/Finance-Hub/FinanceHubMaterials/blob/master/Quantitative%20Finance%20Lectures/classifiers_fx_example.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC()\n",
    "y = np.sign(pooled_data.iloc[:,0])\n",
    "clf.fit(X,y)\n",
    "clf.predict([X0.squeeze()])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the full sample, the SVC is predicting a bear (-1) market in the currency $i$ if its current features are given by the vector $X_{i,T}$.\n",
    "\n",
    "### Predicting with a Decision Tree\n",
    "\n",
    "As we have discussed in a [previous lecture](https://github.com/Finance-Hub/FinanceHubMaterials/blob/master/Quantitative%20Finance%20Lectures/tree_based_methods_in_fx.ipynb), similarly to SVCs, decision trees are constructed by stratifying or segmenting the predictor space into a number of simple regions. In order to make a prediction for a given observation, we typically use the mean or the mode of the training observations in the region to which it belongs. Since the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches are known as decision tree methods Tand are simple and useful for interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth=4)\n",
    "tree_clf.fit(X, y)\n",
    "tree_clf.predict([X0.squeeze()])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the full sample, the Decision Tree is also predicting a bear (-1) market in the currency $i$ if its current features are given by the vector $X_{i,T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "We have seen leave-one-out cross-validation (LOOCV) in a [previous lecture](https://github.com/Finance-Hub/FinanceHubMaterials/blob/master/Quantitative%20Finance%20Lectures/cross_validation_in_fx.ipynb). Here, we use it to create $N=24$ (as many as we have currencies) training sets. Each learning set is created by taking all the markets except one, the test set being the market left out. After the entire process, we will have trained $N$ models, as many models as there are markets or currencies.\n",
    "\n",
    "For each training set we will a prediction for future returns $\\hat{y}_{k,T}$ of currency $i$ given the features vector $X_{i,T}$, one for each trainning set $k$. **Bagging** or **Bootstrap aggregation** consists in taking the average prediction across the different training sets:\n",
    "\n",
    "$$\n",
    "\\hat{y}_{bag,T} = N^{-1}\\sum_{k=1}^{N}\\hat{y}_{k,T}\n",
    "$$\n",
    "\n",
    "There are three main advantages of using bagging for forecasting:\n",
    "\n",
    "1. In practice, we know that it greatly improves prediction accuracy of a statistical learning method. Bagging has been demonstrated to give impressive improvements in accuracy, particularly for decision trees, by combining together hundreds or even thousands of predictions into a single procedure.\n",
    "2. Averaging a set of predictions reduces variance of the predictions so the predictions are more stable across different samples, in particular, the predictions do not completly change as new information becomes available\n",
    "3. Instead of running a model in the full sample, since we are running the model in several independent samples, we can run each bootstrap prediction in parallel, in a seperate CPU using [multiprocessing](https://docs.python.org/2/library/multiprocessing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logo = LeaveOneGroupOut()\n",
    "logo.get_n_splits(groups=groups)\n",
    "results_df = pd.DataFrame()\n",
    "for train_index, test_index in logo.split(X, y, groups.iloc[:,0]):\n",
    "    \n",
    "    group_results = pd.Series(index=['SVC_prediction','SVC_groups_acc','Tree_prediction','Tree_groups_acc'],data=0.)\n",
    "    \n",
    "    clf.fit(X.iloc[train_index],y.iloc[train_index])\n",
    "    group_results['SVC_prediction'] = clf.predict([X0.squeeze()])[0]\n",
    "    group_results['SVC_groups_acc'] = (1*(clf.predict(X.iloc[test_index])==y.iloc[test_index])).mean()\n",
    "                                \n",
    "    tree_clf = tree_clf.fit(X.iloc[train_index], y.iloc[train_index])\n",
    "    group_results['Tree_prediction'] = tree_clf.predict([X0.squeeze()])[0]    \n",
    "    group_results['Tree_groups_acc'] = (1*(tree_clf.predict(X.iloc[test_index])==y.iloc[test_index])).mean()\n",
    "                                           \n",
    "    results_df = results_df.append(group_results.to_frame(groups.iloc[test_index[0],0]).T)                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that now we have two predictions for each trainning set $k$. One prediction $\\hat{y}_{k,T}$ coming from the SVC model and another $\\tilde{y}_{k,T}$ coming from the decision tree model. We also have how they performed in terms of accuracy in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVC_prediction</th>\n",
       "      <th>SVC_groups_acc</th>\n",
       "      <th>Tree_prediction</th>\n",
       "      <th>Tree_groups_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.553324</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.516287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.555109</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.543507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.496653</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.495761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.499777</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.541276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.551986</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.521196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.531958</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.589331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.562695</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.566265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.572959</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.565373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.603748</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.635877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.532352</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.526551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.557787</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.569389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.505578</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.494868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.512271</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.535029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.467648</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.502900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.547970</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.544400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.554663</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.560018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.501116</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.509594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.562249</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.538599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.507363</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.529674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.534137</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.553324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.490852</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.495761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.526104</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.585899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.522535</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.548862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.498884</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.543507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SVC_prediction  SVC_groups_acc  Tree_prediction  Tree_groups_acc\n",
       "0             -1.0        0.553324             -1.0         0.516287\n",
       "1             -1.0        0.555109             -1.0         0.543507\n",
       "2             -1.0        0.496653             -1.0         0.495761\n",
       "3             -1.0        0.499777             -1.0         0.541276\n",
       "4             -1.0        0.551986             -1.0         0.521196\n",
       "5             -1.0        0.531958             -1.0         0.589331\n",
       "6             -1.0        0.562695             -1.0         0.566265\n",
       "7             -1.0        0.572959             -1.0         0.565373\n",
       "8             -1.0        0.603748             -1.0         0.635877\n",
       "9             -1.0        0.532352             -1.0         0.526551\n",
       "10            -1.0        0.557787             -1.0         0.569389\n",
       "11            -1.0        0.505578             -1.0         0.494868\n",
       "12            -1.0        0.512271             -1.0         0.535029\n",
       "13            -1.0        0.467648             -1.0         0.502900\n",
       "14            -1.0        0.547970             -1.0         0.544400\n",
       "15            -1.0        0.554663             -1.0         0.560018\n",
       "16            -1.0        0.501116             -1.0         0.509594\n",
       "17            -1.0        0.562249             -1.0         0.538599\n",
       "18            -1.0        0.507363             -1.0         0.529674\n",
       "19            -1.0        0.534137             -1.0         0.553324\n",
       "20            -1.0        0.490852             -1.0         0.495761\n",
       "21            -1.0        0.526104             -1.0         0.585899\n",
       "22            -1.0        0.522535             -1.0         0.548862\n",
       "23            -1.0        0.498884             -1.0         0.543507"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the simple bagging prediction with SVC: -1.0\n",
      "This is the simple bagging prediction with DT: -1.0\n",
      "This is the weighted bagging prediction with SVC: -0.9999999999999999\n",
      "This is the weighted bagging prediction with Tree: -1.0\n",
      "This is the weighted bagging prediction with Tree + SVC combined: -1.0\n"
     ]
    }
   ],
   "source": [
    "print('This is the simple bagging prediction with SVC: %s' % results_df['SVC_prediction'].mean())\n",
    "print('This is the simple bagging prediction with DT: %s' % results_df['Tree_prediction'].mean())\n",
    "acc_weights = results_df['SVC_groups_acc']/results_df['SVC_groups_acc'].sum()\n",
    "weighted_SVC = results_df['SVC_prediction']*acc_weights\n",
    "print('This is the weighted bagging prediction with SVC: %s' % weighted_SVC.sum())\n",
    "acc_weights = results_df['Tree_groups_acc']/results_df['Tree_groups_acc'].sum()\n",
    "weighted_tree = results_df['Tree_prediction']*acc_weights\n",
    "print('This is the weighted bagging prediction with Tree: %s' % weighted_tree.sum())\n",
    "model_combination = (weighted_SVC+weighted_tree)/2\n",
    "print('This is the weighted bagging prediction with Tree + SVC combined: %s' % model_combination.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above are not exactly predictions, they are averages of -1 and 1 predictions. So, to predict a quantitative outcome for $y_{i,T} = r_{i,T+h}$ we typically take a **majority vote** approach by making the overall prediction is the most commonly occurring majority class among the predictions. In our case, this would be a bull market (1) if the bagging prediction was positive and a bear market (-1) if the bagging prediction was negative.\n",
    "\n",
    "## Boosting\n",
    "\n",
    "The main differece between bagging and boosting is that boosting is sequential. While we start with one model for prediction in the first train set, we will fit the model in the second train set taking into account what we have learned in the previous train set. The second model will try to correct the mistakes of the previous model.\n",
    "\n",
    "Boosting is particularly important in tree based methods making them popular in all kinds of data science problems after successfull performance in [forecasting competitions](https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470996430.ch15) such as the [M3-Competitions](https://en.wikipedia.org/wiki/Makridakis_Competitions) and more recently on [Kaggle competitions](https://www.kaggle.com/competitions). These very succesfull algorithms, such as the [XGBoost](https://syncedreview.com/2017/10/22/tree-boosting-with-xgboost-why-does-xgboost-win-every-machine-learning-competition/) which we will cover in a [future lecture](https://github.com/Finance-Hub/FinanceHubMaterials/blob/master/Quantitative%20Finance%20Lectures/momentum_with_regression_trees.ipynb) use resampling methods such as the ones we discussed in [previous lectures](https://github.com/Finance-Hub/FinanceHubMaterials/blob/master/Quantitative%20Finance%20Lectures/cross_validation_in_fx.ipynb) to combine a large number of decision trees, resulting in great in prediction accuracy.\n",
    "\n",
    "Here, we describe one simple boosting methods for classifiers, [AdaBoost](https://scikit-learn.org/stable/modules/ensemble.html#adaboost). [AdaBoost](https://scikit-learn.org/stable/modules/ensemble.html#adaboost) begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\n",
    "\n",
    "#### AdaBoost\n",
    "\n",
    "The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights to each of the training samples. Initially, those weights are all set uniformly so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence.\n",
    "\n",
    "Using AdaBoost in the full sample is simple like any other classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=200,learning_rate=0.5,algorithm=\"SAMME.R\")\n",
    "ada_clf.fit(X, y)\n",
    "ada_clf.predict([X0.squeeze()])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how each of the AdaBoost predictions do out of sample using the LOOCV method. Each learning set is created by taking all the markets except one, the test set being the market left out. After the entire process, we will have trained $N$ AdaBoost classifiers, as many classifiers as there are markets or currencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logo = LeaveOneGroupOut()\n",
    "logo.get_n_splits(groups=groups)\n",
    "adaboost_results_df = pd.DataFrame()\n",
    "for train_index, test_index in logo.split(X, y, groups.iloc[:,0]):    \n",
    "    group_results = pd.Series(index=['AdaBoost_prediction','AdaBoost_groups_acc'],data=0.)\n",
    "    \n",
    "    ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=200,learning_rate=0.5,algorithm=\"SAMME.R\")\n",
    "    ada_clf = ada_clf.fit(X.iloc[train_index],y.iloc[train_index])    \n",
    "    \n",
    "    group_results['AdaBoost_prediction'] = ada_clf.predict([X0.squeeze()])[0]\n",
    "    group_results['AdaBoost_groups_acc'] = (1*(ada_clf.predict(X.iloc[test_index])==y.iloc[test_index])).mean()\n",
    "    \n",
    "    adaboost_results_df = adaboost_results_df.append(group_results.to_frame(groups.iloc[test_index[0],0]).T)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdaBoost_prediction</th>\n",
       "      <th>AdaBoost_groups_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.533690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.547077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.488621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.495761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.534583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.557121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.544846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.567158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.593039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.514056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.532798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.497100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.522088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.493530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.553771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.546185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.507809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.543954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.511825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.544400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.498884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.553771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.492191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.504239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AdaBoost_prediction  AdaBoost_groups_acc\n",
       "0                  -1.0             0.533690\n",
       "1                  -1.0             0.547077\n",
       "2                  -1.0             0.488621\n",
       "3                  -1.0             0.495761\n",
       "4                  -1.0             0.534583\n",
       "5                  -1.0             0.557121\n",
       "6                  -1.0             0.544846\n",
       "7                  -1.0             0.567158\n",
       "8                  -1.0             0.593039\n",
       "9                  -1.0             0.514056\n",
       "10                 -1.0             0.532798\n",
       "11                 -1.0             0.497100\n",
       "12                 -1.0             0.522088\n",
       "13                 -1.0             0.493530\n",
       "14                 -1.0             0.553771\n",
       "15                 -1.0             0.546185\n",
       "16                 -1.0             0.507809\n",
       "17                 -1.0             0.543954\n",
       "18                 -1.0             0.511825\n",
       "19                 -1.0             0.544400\n",
       "20                 -1.0             0.498884\n",
       "21                 -1.0             0.553771\n",
       "22                 -1.0             0.492191\n",
       "23                 -1.0             0.504239"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "One of the problems with these tree-based methods, particularly when using bagging or boosting is that we lack a bit of interpretation on the importance of each regressor or feature in the predictive model. Let’s start with decision trees to build some intuition. In decision trees, every node is a condition of how to split values in a single feature, so that similar values of the dependent variable end up in the same set after the split. The condition is based on impurity, which in case of classification problems is Gini impurity/information gain (entropy), while for regression trees its variance. So when training a tree we can compute how much each feature contributes to decreasing the weighted impurity.\n",
    "\n",
    "Let's use the method `feature_importances_` in `Scikit-Learn` to implement that logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                         class_weight=None,\n",
       "                                                         criterion='gini',\n",
       "                                                         max_depth=1,\n",
       "                                                         max_features=None,\n",
       "                                                         max_leaf_nodes=None,\n",
       "                                                         min_impurity_decrease=0.0,\n",
       "                                                         min_impurity_split=None,\n",
       "                                                         min_samples_leaf=1,\n",
       "                                                         min_samples_split=2,\n",
       "                                                         min_weight_fraction_leaf=0.0,\n",
       "                                                         presort='deprecated',\n",
       "                                                         random_state=None,\n",
       "                                                         splitter='best'),\n",
       "                   learning_rate=0.5, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=200,learning_rate=0.5,algorithm=\"SAMME.R\")\n",
    "ada_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Feature Importance on AdaBoost')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "pd.Series(index=X.columns,data=ada_clf.feature_importances_).sort_values().plot(kind='bar')\n",
    "plt.title('Feature Importance on AdaBoost')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
